{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN + Transfer learning for Centipede -> SpaceInvaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Conv2D, MaxPool2D\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "def to_grayscale(img):\n",
    "    return np.mean(img, axis=2).astype(np.uint8)\n",
    "\n",
    "def downsample(img):\n",
    "    return img[::2, ::2]\n",
    "\n",
    "def preprocess(img):\n",
    "    if img.shape == (250, 160, 3):\n",
    "        return to_grayscale(downsample(img))\n",
    "    else:\n",
    "        img2 = cv2.resize(img, (80, 125))\n",
    "        return to_grayscale(img2)\n",
    "\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, env, loaded, freeze_on, num_freeze):\n",
    "        self.env     = env\n",
    "        self.memory  = deque(maxlen=500) # appends to queue and pops at the other end\n",
    "        \n",
    "        self.gamma = 0.85\n",
    "        self.freeze_on = freeze_on\n",
    "        self.loaded = loaded\n",
    "        self.num_freeze = num_freeze\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.02\n",
    "        self.tau = .125\n",
    "        if self.loaded == None:\n",
    "            self.model        = self.create_model()\n",
    "            self.target_model = self.create_model()\n",
    "        else:\n",
    "            self.model = self.transfer_model()\n",
    "            self.target_model = self.transfer_model()\n",
    "        \n",
    "    def freeze_layer(self, num):\n",
    "        for layer in self.model.layers[:num]:\n",
    "            layer.trainable = False\n",
    "        for layer in self.target_model.layers[:num]:\n",
    "            layer.trainable = False\n",
    "        self.model.compile(loss=\"mean_squared_error\",\n",
    "            optimizer=Adam(lr=self.learning_rate))\n",
    "        self.target_model.compile(loss=\"mean_squared_error\",\n",
    "            optimizer=Adam(lr=self.learning_rate))\n",
    "        hidden = Dense(120, activation='relu')(model.layers[-2].output). model.layers[-1].output\n",
    "\n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(125, 80, 1)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(16, kernel_size=(3, 3), activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(self.env.action_space.n))\n",
    "        model.compile(loss=\"mean_squared_error\",\n",
    "            optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def transfer_model(self):\n",
    "        trans_model = load_model(self.loaded)\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(125, 80, 1), \n",
    "                         weights=trans_model.layers[0].get_weights()))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', weights=trans_model.layers[2].get_weights()))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(self.env.action_space.n))\n",
    "        if self.freeze_on:\n",
    "             for layer in model.layers[:self.num_freeze]:\n",
    "                    layer.trainable = False\n",
    "        model.compile(loss=\"mean_squared_error\",\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    # take action\n",
    "    def act(self, state):\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon)\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        state = preprocess(state).reshape((1, 125,80,1))\n",
    "        return np.argmax(self.model.predict(state)[0])\n",
    "\n",
    "    # remember history\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.append([state, action, reward, new_state, done])\n",
    "\n",
    "    # memory\n",
    "    def replay(self):\n",
    "        batch_size = 32\n",
    "        if len(self.memory) < batch_size: \n",
    "            return\n",
    "\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        for sample in samples:\n",
    "            state, action, reward, new_state, done = sample\n",
    "            new_state = preprocess(new_state).reshape((1, 125,80,1))\n",
    "            state = preprocess(state).reshape((1, 125,80,1))\n",
    "            target = self.target_model.predict(state)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                Q_future = max(self.target_model.predict(new_state)[0])\n",
    "                target[0][action] = reward + Q_future * self.gamma\n",
    "            self.model.fit(state, target, epochs=1, verbose=0)\n",
    "\n",
    "    def target_train(self):\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i] = weights[i] * self.tau + target_weights[i] * (1 - self.tau)\n",
    "        self.target_model.set_weights(target_weights)\n",
    "\n",
    "    def save_model(self, fn):\n",
    "        self.model.save(fn)\n",
    "\n",
    "    def save_model(self, fn):\n",
    "        self.model.save(fn)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Died with 215.0 points\n",
      "Died with 195.0 points\n",
      "Died with 125.0 points\n",
      "Died with 155.0 points\n",
      "Died with 235.0 points\n",
      "Died with 90.0 points\n",
      "Died with 280.0 points\n",
      "Died with 200.0 points\n",
      "Died with 210.0 points\n",
      "Died with 85.0 points\n",
      "Died with 300.0 points\n",
      "Died with 90.0 points\n",
      "Died with 50.0 points\n",
      "Died with 180.0 points\n",
      "Died with 75.0 points\n",
      "Died with 210.0 points\n",
      "Died with 105.0 points\n",
      "Died with 95.0 points\n",
      "Died with 100.0 points\n",
      "Died with 85.0 points\n",
      "Died with 230.0 points\n",
      "Died with 140.0 points\n",
      "Died with 220.0 points\n",
      "Died with 130.0 points\n",
      "Died with 290.0 points\n",
      "Died with 140.0 points\n",
      "Died with 220.0 points\n",
      "Died with 180.0 points\n",
      "Died with 95.0 points\n",
      "Died with 105.0 points\n",
      "Died with 135.0 points\n",
      "Died with 75.0 points\n",
      "Died with 105.0 points\n",
      "Died with 110.0 points\n",
      "Died with 275.0 points\n",
      "Died with 410.0 points\n",
      "Died with 370.0 points\n",
      "Died with 260.0 points\n",
      "Died with 205.0 points\n",
      "Died with 105.0 points\n",
      "Died with 335.0 points\n",
      "Died with 135.0 points\n",
      "Died with 305.0 points\n",
      "Died with 270.0 points\n",
      "Died with 40.0 points\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5a33706b0d4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# store in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mdqn_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mdqn_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m       \u001b[1;31m# internally iterates default (prediction) model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;31m#print(reward)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mdqn_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# iterates target model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-46501b2ab671>\u001b[0m in \u001b[0;36mreplay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[0mnew_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m125\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m125\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36_2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1169\u001b[1;33m                                             steps=steps)\n\u001b[0m\u001b[0;32m   1170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36_2\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36_2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36_2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36_2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#def main():\n",
    "env     = gym.make(\"SpaceInvaders-v0\")\n",
    "gamma   = 0.95\n",
    "epsilon = .95\n",
    "trials  = 10000\n",
    "num_freeze = 1\n",
    "freeze = True\n",
    "#trial_len = 500\n",
    "\n",
    "# updateTargetNetwork = 1000\n",
    "dqn_agent = DQN(env=env, loaded='dqn_centipede.model', freeze_on = True, num_freeze=num_freeze)\n",
    "rewards = []\n",
    "for trial in range(trials):\n",
    "    cur_state = env.reset()#.reshape(1,2)  (250, 160, 3)\n",
    "    #cur_state = preprocess(cur_state).reshape((125,80,1))\n",
    "    reward_new = 0\n",
    "    while True:\n",
    "        # take action from current state (epsilon greedy)\n",
    "        action = dqn_agent.act(cur_state)\n",
    "        # take one step\n",
    "        new_state, reward, done, life = env.step(action)\n",
    "        reward_new += reward\n",
    "        #new_state = preprocess(new_state).reshape((125,80,1))\n",
    "        # reward = reward if not done else -20\n",
    "        #new_state = new_state.reshape(1,2)\n",
    "        # store in memory\n",
    "        dqn_agent.remember(cur_state, action, reward, new_state, done)\n",
    "        dqn_agent.replay()       # internally iterates default (prediction) model\n",
    "        #print(reward)\n",
    "        dqn_agent.target_train() # iterates target model\n",
    "        cur_state = new_state\n",
    "        if done:\n",
    "            break\n",
    "    rewards.append(reward_new)\n",
    "    print(\"Died with {} points\".format(reward_new))\n",
    "    dqn_agent.save_model(\"dqn_centipede_transfer_spaceinvaders.model\")\n",
    "    #dqn_agent.save_model(\"dqn_centipede.model\")\n",
    "\n",
    "\n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[215.0,\n",
       " 195.0,\n",
       " 125.0,\n",
       " 155.0,\n",
       " 235.0,\n",
       " 90.0,\n",
       " 280.0,\n",
       " 200.0,\n",
       " 210.0,\n",
       " 85.0,\n",
       " 300.0,\n",
       " 90.0,\n",
       " 50.0,\n",
       " 180.0,\n",
       " 75.0,\n",
       " 210.0,\n",
       " 105.0,\n",
       " 95.0,\n",
       " 100.0,\n",
       " 85.0,\n",
       " 230.0,\n",
       " 140.0,\n",
       " 220.0,\n",
       " 130.0,\n",
       " 290.0,\n",
       " 140.0,\n",
       " 220.0,\n",
       " 180.0,\n",
       " 95.0,\n",
       " 105.0,\n",
       " 135.0,\n",
       " 75.0,\n",
       " 105.0,\n",
       " 110.0,\n",
       " 275.0,\n",
       " 410.0,\n",
       " 370.0,\n",
       " 260.0,\n",
       " 205.0,\n",
       " 105.0,\n",
       " 335.0,\n",
       " 135.0,\n",
       " 305.0,\n",
       " 270.0,\n",
       " 40.0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

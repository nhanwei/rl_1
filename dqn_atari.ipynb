{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Conv2D, MaxPool2D\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "def to_grayscale(img):\n",
    "    return np.mean(img, axis=2).astype(np.uint8)\n",
    "\n",
    "def downsample(img):\n",
    "    return img[::2, ::2]\n",
    "\n",
    "def preprocess(img):\n",
    "    if img.shape == (250, 160, 3):\n",
    "        return to_grayscale(downsample(img))\n",
    "    else:\n",
    "        img2 = cv2.resize(img, (80, 125))\n",
    "        return to_grayscale(img2)\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, env):\n",
    "        self.env     = env\n",
    "        self.memory  = deque(maxlen=500) # appends to queue and pops at the other end\n",
    "        \n",
    "        self.gamma = 0.85\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.02\n",
    "        self.tau = .125\n",
    "\n",
    "        self.model        = self.create_model()\n",
    "        self.target_model = self.create_model()\n",
    "\n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(125, 80, 1)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(16, kernel_size=(3, 3), activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(self.env.action_space.n))\n",
    "        model.compile(loss=\"mean_squared_error\",\n",
    "            optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    # take action\n",
    "    def act(self, state):\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon)\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        state = preprocess(state).reshape((1, 125,80,1))\n",
    "        return np.argmax(self.model.predict(state)[0])\n",
    "\n",
    "    # remember history\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.append([state, action, reward, new_state, done])\n",
    "\n",
    "    # memory\n",
    "    def replay(self):\n",
    "        batch_size = 32\n",
    "        if len(self.memory) < batch_size: \n",
    "            return\n",
    "\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        for sample in samples:\n",
    "            state, action, reward, new_state, done = sample\n",
    "            new_state = preprocess(new_state).reshape((1, 125,80,1))\n",
    "            state = preprocess(state).reshape((1, 125,80,1))\n",
    "            target = self.target_model.predict(state)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                Q_future = max(self.target_model.predict(new_state)[0])\n",
    "                target[0][action] = reward + Q_future * self.gamma\n",
    "            self.model.fit(state, target, epochs=1, verbose=0)\n",
    "\n",
    "    def target_train(self):\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i] = weights[i] * self.tau + target_weights[i] * (1 - self.tau)\n",
    "        self.target_model.set_weights(target_weights)\n",
    "\n",
    "    def save_model(self, fn):\n",
    "        self.model.save(fn)\n",
    "\n",
    "\n",
    "env     = gym.make(\"Centipede-v0\")\n",
    "gamma   = 0.95\n",
    "epsilon = .95\n",
    "trials  = 10000\n",
    "\n",
    "\n",
    "dqn_agent = DQN(env=env)\n",
    "rewards = []\n",
    "for trial in range(trials):\n",
    "    cur_state = env.reset()#.reshape(1,2)  (250, 160, 3)\n",
    "    reward_new = 0\n",
    "    while True:\n",
    "        # take action from current state (epsilon greedy)\n",
    "        action = dqn_agent.act(cur_state)\n",
    "        # take one step\n",
    "        new_state, reward, done, life = env.step(action)\n",
    "        reward_new += reward\n",
    "        # reward = reward if not done else -20\n",
    "        # store in memory\n",
    "        dqn_agent.remember(cur_state, action, reward, new_state, done)\n",
    "        dqn_agent.replay()       # internally iterates default (prediction) model\n",
    "        #print(reward)\n",
    "        dqn_agent.target_train() # iterates target model\n",
    "        cur_state = new_state\n",
    "        if done:\n",
    "            break\n",
    "    rewards.append(reward_new)\n",
    "    print(\"Died with {} points\".format(reward_new))\n",
    "    dqn_agent.save_model(\"dqn_centipede.model\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
